{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16edffc0-c5f5-44c8-9cc7-f5e31efc3a16",
   "metadata": {},
   "source": [
    "## Imports and accessing LLM, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189b7c4f-3487-49df-b9d1-ccd2ec62c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sulav\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "# Add the 'evaluation' folder to the Python path\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(current_dir, 'evaluation'))\n",
    "sys.path.append(os.path.join(current_dir, 'data'))\n",
    "\n",
    "from generate_gradient import generate_textual_gradient\n",
    "from evaluators.evaluation import evaluate_functional_correctness\n",
    "from evaluators.evaluation import read_problems,stream_jsonl, write_jsonl\n",
    "\n",
    "import google.generativeai as genai\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfaafb37-1e8b-455a-949b-898ccd04f614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gemini_key = \"AIzaSyC8TjivmylsAQPaCyOhdaEWS1Ktt-8QqSQ\"\n",
    "genai.configure(api_key=gemini_key)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/openai/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b2e6f9-2123-45bd-89d3-109c23e37e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>canonical_solution</th>\n",
       "      <th>test</th>\n",
       "      <th>entry_point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HumanEval/0</td>\n",
       "      <td>from typing import List\\n\\n\\ndef has_close_ele...</td>\n",
       "      <td>for idx, elem in enumerate(numbers):\\n    ...</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>has_close_elements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HumanEval/1</td>\n",
       "      <td>from typing import List\\n\\n\\ndef separate_pare...</td>\n",
       "      <td>result = []\\n    current_string = []\\n    ...</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>separate_paren_groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HumanEval/2</td>\n",
       "      <td>\\n\\ndef truncate_number(number: float) -&gt; floa...</td>\n",
       "      <td>return number % 1.0\\n</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>truncate_number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HumanEval/3</td>\n",
       "      <td>from typing import List\\n\\n\\ndef below_zero(op...</td>\n",
       "      <td>balance = 0\\n\\n    for op in operations:\\n...</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>below_zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HumanEval/4</td>\n",
       "      <td>from typing import List\\n\\n\\ndef mean_absolute...</td>\n",
       "      <td>mean = sum(numbers) / len(numbers)\\n    re...</td>\n",
       "      <td>\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...</td>\n",
       "      <td>mean_absolute_deviation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>HumanEval/159</td>\n",
       "      <td>\\ndef eat(number, need, remaining):\\n    \"\"\"\\n...</td>\n",
       "      <td>if(need &lt;= remaining):\\n        return [ n...</td>\n",
       "      <td>def check(candidate):\\n\\n    # Check some simp...</td>\n",
       "      <td>eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>HumanEval/160</td>\n",
       "      <td>\\ndef do_algebra(operator, operand):\\n    \"\"\"\\...</td>\n",
       "      <td>expression = str(operand[0])\\n    for oprt...</td>\n",
       "      <td>def check(candidate):\\n\\n    # Check some simp...</td>\n",
       "      <td>do_algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>HumanEval/161</td>\n",
       "      <td>\\ndef solve(s):\\n    \"\"\"You are given a string...</td>\n",
       "      <td>flg = 0\\n    idx = 0\\n    new_str = list(s...</td>\n",
       "      <td>def check(candidate):\\n\\n    # Check some simp...</td>\n",
       "      <td>solve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>HumanEval/162</td>\n",
       "      <td>\\ndef string_to_md5(text):\\n    \"\"\"\\n    Given...</td>\n",
       "      <td>import hashlib\\n    return hashlib.md5(tex...</td>\n",
       "      <td>def check(candidate):\\n\\n    # Check some simp...</td>\n",
       "      <td>string_to_md5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>HumanEval/163</td>\n",
       "      <td>\\ndef generate_integers(a, b):\\n    \"\"\"\\n    G...</td>\n",
       "      <td>lower = max(2, min(a, b))\\n    upper = min...</td>\n",
       "      <td>def check(candidate):\\n\\n    # Check some simp...</td>\n",
       "      <td>generate_integers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           task_id                                             prompt  \\\n",
       "0      HumanEval/0  from typing import List\\n\\n\\ndef has_close_ele...   \n",
       "1      HumanEval/1  from typing import List\\n\\n\\ndef separate_pare...   \n",
       "2      HumanEval/2  \\n\\ndef truncate_number(number: float) -> floa...   \n",
       "3      HumanEval/3  from typing import List\\n\\n\\ndef below_zero(op...   \n",
       "4      HumanEval/4  from typing import List\\n\\n\\ndef mean_absolute...   \n",
       "..             ...                                                ...   \n",
       "159  HumanEval/159  \\ndef eat(number, need, remaining):\\n    \"\"\"\\n...   \n",
       "160  HumanEval/160  \\ndef do_algebra(operator, operand):\\n    \"\"\"\\...   \n",
       "161  HumanEval/161  \\ndef solve(s):\\n    \"\"\"You are given a string...   \n",
       "162  HumanEval/162  \\ndef string_to_md5(text):\\n    \"\"\"\\n    Given...   \n",
       "163  HumanEval/163  \\ndef generate_integers(a, b):\\n    \"\"\"\\n    G...   \n",
       "\n",
       "                                    canonical_solution  \\\n",
       "0        for idx, elem in enumerate(numbers):\\n    ...   \n",
       "1        result = []\\n    current_string = []\\n    ...   \n",
       "2                                return number % 1.0\\n   \n",
       "3        balance = 0\\n\\n    for op in operations:\\n...   \n",
       "4        mean = sum(numbers) / len(numbers)\\n    re...   \n",
       "..                                                 ...   \n",
       "159      if(need <= remaining):\\n        return [ n...   \n",
       "160      expression = str(operand[0])\\n    for oprt...   \n",
       "161      flg = 0\\n    idx = 0\\n    new_str = list(s...   \n",
       "162      import hashlib\\n    return hashlib.md5(tex...   \n",
       "163      lower = max(2, min(a, b))\\n    upper = min...   \n",
       "\n",
       "                                                  test  \\\n",
       "0    \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "1    \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "2    \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "3    \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "4    \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...   \n",
       "..                                                 ...   \n",
       "159  def check(candidate):\\n\\n    # Check some simp...   \n",
       "160  def check(candidate):\\n\\n    # Check some simp...   \n",
       "161  def check(candidate):\\n\\n    # Check some simp...   \n",
       "162  def check(candidate):\\n\\n    # Check some simp...   \n",
       "163  def check(candidate):\\n\\n    # Check some simp...   \n",
       "\n",
       "                 entry_point  \n",
       "0         has_close_elements  \n",
       "1      separate_paren_groups  \n",
       "2            truncate_number  \n",
       "3                 below_zero  \n",
       "4    mean_absolute_deviation  \n",
       "..                       ...  \n",
       "159                      eat  \n",
       "160               do_algebra  \n",
       "161                    solve  \n",
       "162            string_to_md5  \n",
       "163        generate_integers  \n",
       "\n",
       "[164 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcbdd24b-d9dd-4c3e-b976-867b9e73f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(dataframe) -> List[dict]:\n",
    "    responses = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        task_id = row.get('task_id')\n",
    "        prompt = row.get('prompt')\n",
    "        try:\n",
    "            generated_soln = model.generate_content(prompt)\n",
    "            if generated_soln.candidates[0].finish_reason == 1:\n",
    "                print(f\"{task_id} done\")\n",
    "                responses.append(\n",
    "                {\n",
    "                    \"task_id\": task_id,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"solution\": solution,\n",
    "                    \"generated_output\": generated_soln.text,\n",
    "                }\n",
    "            )\n",
    "            elif generated_soln.candidates[0].finish_reason == 4:\n",
    "                print(f\"{task_id} generating copyrighted material\")\n",
    "                failed_responses.append(\n",
    "                {\n",
    "                    \"task_id\": task_id,\n",
    "                    \"prompt\" : prompt,\n",
    "                    \"solution\" : solution,\n",
    "                    \"failed_reason\": \"Reciting Copyrighted material\"\n",
    "                }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response for prompt: {prompt}\\n{e}\")\n",
    "            responses.append({\"task_id\":row.task_id, \"prompt\": prompt, \"generated_output\": \"\"})\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba7acd9f-cd4c-4243-a073-62a805c56419",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ca5c72-955d-42ad-91ff-7721fe120ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df['prompt'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eff1f1-41fa-4ac9-900a-ed22c2da8366",
   "metadata": {},
   "source": [
    "## Defining Po, Dtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb216774-0ac5-4721-9270-79c5d8cafcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Po = \"\"\"f'Implement the function function_name that solves the following problem:\n",
    "\n",
    "Problem statement: {Problem description}\n",
    "\n",
    "Function signature: def function_name(*args) -> return_type:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e563dc55-1653-4cad-87fe-ae92e96045df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtr = df[[\"task_id\",\"prompt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917492c1-db0f-4072-abb5-6fd0ca1f168c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HumanEval/0</td>\n",
       "      <td>from typing import List\\n\\n\\ndef has_close_ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HumanEval/1</td>\n",
       "      <td>from typing import List\\n\\n\\ndef separate_pare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HumanEval/2</td>\n",
       "      <td>\\n\\ndef truncate_number(number: float) -&gt; floa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HumanEval/3</td>\n",
       "      <td>from typing import List\\n\\n\\ndef below_zero(op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HumanEval/4</td>\n",
       "      <td>from typing import List\\n\\n\\ndef mean_absolute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>HumanEval/159</td>\n",
       "      <td>\\ndef eat(number, need, remaining):\\n    \"\"\"\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>HumanEval/160</td>\n",
       "      <td>\\ndef do_algebra(operator, operand):\\n    \"\"\"\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>HumanEval/161</td>\n",
       "      <td>\\ndef solve(s):\\n    \"\"\"You are given a string...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>HumanEval/162</td>\n",
       "      <td>\\ndef string_to_md5(text):\\n    \"\"\"\\n    Given...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>HumanEval/163</td>\n",
       "      <td>\\ndef generate_integers(a, b):\\n    \"\"\"\\n    G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           task_id                                             prompt\n",
       "0      HumanEval/0  from typing import List\\n\\n\\ndef has_close_ele...\n",
       "1      HumanEval/1  from typing import List\\n\\n\\ndef separate_pare...\n",
       "2      HumanEval/2  \\n\\ndef truncate_number(number: float) -> floa...\n",
       "3      HumanEval/3  from typing import List\\n\\n\\ndef below_zero(op...\n",
       "4      HumanEval/4  from typing import List\\n\\n\\ndef mean_absolute...\n",
       "..             ...                                                ...\n",
       "159  HumanEval/159  \\ndef eat(number, need, remaining):\\n    \"\"\"\\n...\n",
       "160  HumanEval/160  \\ndef do_algebra(operator, operand):\\n    \"\"\"\\...\n",
       "161  HumanEval/161  \\ndef solve(s):\\n    \"\"\"You are given a string...\n",
       "162  HumanEval/162  \\ndef string_to_md5(text):\\n    \"\"\"\\n    Given...\n",
       "163  HumanEval/163  \\ndef generate_integers(a, b):\\n    \"\"\"\\n    G...\n",
       "\n",
       "[164 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27ecd6d-209c-4b77-8eb1-5d0068f59cab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_to_dict(df) -> List[dict]:\n",
    "    return df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287b6bc-efe1-4f44-bda8-36f0ad555abe",
   "metadata": {},
   "source": [
    "## Main Function (ProTeGi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "704a5801-bfa5-497a-a5d4-d11fb7d39564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_prompt(Po):\n",
    "    C = []\n",
    "    C.extend(expand(Po,Dtr))\n",
    "    prompt_candidates = C\n",
    "    print(prompt_candidates)\n",
    "    optimal_prompt = best_prompt(prompt_candidates) # Find the best prompt from the final beam\n",
    "    return optimal_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac81559-32a5-4a8d-8e02-d2c2fe060160",
   "metadata": {},
   "source": [
    "## Expansion Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e158d",
   "metadata": {},
   "source": [
    "### Loading the transformer model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db297612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"../../CodeT5/fine_tuned_codet5\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ac8ba",
   "metadata": {},
   "source": [
    "### Expansion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bde00f50-27c0-48f6-87ef-b2ef6e17f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_minibatch(df: pd.DataFrame, batch_size: int) -> pd.DataFrame:\n",
    "    return df.sample(n=batch_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f286cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_error(input_text, model, tokenizer, device):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate output using the model\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=256,  # Adjust this as needed\n",
    "        temperature=0.7,  # Sampling temperature for diversity\n",
    "        num_return_sequences=1,  # Number of responses to generate\n",
    "        top_k=50,  # Use top-k sampling\n",
    "        top_p=0.95  # Use nucleus sampling\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849f358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(Po,Dtr):\n",
    "    batch_size = 10\n",
    "    D_mini_batch = sample_minibatch(Dtr,batch_size)\n",
    "    \n",
    "    def fill_prompt(row):\n",
    "        problem_description = row.get(\"prompt\", \"\")\n",
    "        # Insert them into Po\n",
    "        merged_prompt = (\n",
    "            Po.replace(\"{Problem description}\", problem_description)\n",
    "        )\n",
    "        return merged_prompt\n",
    "    problems = read_problems()\n",
    "    D_mini_batch[\"prompt\"] = D_mini_batch.apply(fill_prompt, axis=1)\n",
    "    \n",
    "    responses = generate_responses(D_mini_batch)\n",
    "    \n",
    "    prompt_candidates = []\n",
    "    \n",
    "    for response in responses:\n",
    "        problem_id = response.get(\"task_id\")\n",
    "        generated_output = response.get(\"generated_output\")\n",
    "        expected_output = problems[problem_id][\"canonical_solution\"]\n",
    "        input_text = (\n",
    "            f\"\"\"Generated_output: {generated_output}, expected_output: {expected_output} \"\"\"\n",
    "            f\"\"\"What is the error or difference?\"\"\"\n",
    "        )\n",
    "        error = generate_error(input_text, model, tokenizer, device) \n",
    "        gradient = generate_textual_gradient(error, use_similarity=True)\n",
    "        edited_prompt = (Po + \"/t\" + gradient)\n",
    "        prompt_candidates.append({\n",
    "        \"task_id\":problem_id,\n",
    "        \"prompt\": edited_prompt\n",
    "        }\n",
    "        )\n",
    "        \n",
    "    return prompt_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "255d2154-f46d-4e43-b48b-1b4ebb621806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_prompt(prompt_candidates, sample_file=\"sample_file.jsonl\", problem_file=\"HUMAN_EVAL\"):\n",
    "    \"\"\"\n",
    "    Evaluates the pass@k for each prompt candidate and returns the prompt with the highest pass@k value.\n",
    "    \n",
    "    Args:\n",
    "        prompt_candidates (list): List of dictionaries with the format {\"task_id\": problem_id, \"prompt\": edited_prompt}.\n",
    "        sample_file (str): Path to save generated responses for evaluation.\n",
    "        problem_file (str): Path to the problem definitions.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The prompt candidate with the highest pass@k value.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Write prompt candidates to the sample file\n",
    "    write_jsonl(filename=sample_file, data=prompt_candidates)\n",
    "\n",
    "    # Evaluate functional correctness\n",
    "    pass_at_k = evaluate_functional_correctness(sample_file, k=[1, 10, 100], n_workers=4, timeout=3.0, problem_file=problem_file)\n",
    "\n",
    "    # Load problems for error analysis (optional)\n",
    "    problems = read_problems(problem_file)\n",
    "\n",
    "    # Analyze errors and collect pass@k values\n",
    "    best_prompt = None\n",
    "    best_pass_at_k = -1  # Initialize with the lowest possible value\n",
    "\n",
    "    for candidate in prompt_candidates:\n",
    "        task_id = candidate.get(\"task_id\")\n",
    "        prompt = candidate.get(\"prompt\")\n",
    "        \n",
    "        # Filter pass@k for this specific task\n",
    "        task_pass_at_k = {key: pass_at_k[key] for key in pass_at_k if str(task_id) in key}\n",
    "\n",
    "        # Compute average pass@k (or select the metric you want to optimize)\n",
    "        avg_pass_at_k = sum(task_pass_at_k.values()) / len(task_pass_at_k) if task_pass_at_k else 0\n",
    "\n",
    "        # Update the best prompt based on pass@k\n",
    "        if avg_pass_at_k > best_pass_at_k:\n",
    "            best_pass_at_k = avg_pass_at_k\n",
    "            best_prompt = {\n",
    "                \"task_id\": task_id,\n",
    "                \"prompt\": prompt,\n",
    "                \"pass@k\": task_pass_at_k,\n",
    "                \"average_pass@k\": avg_pass_at_k\n",
    "            }\n",
    "\n",
    "    return best_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79d1b8",
   "metadata": {},
   "source": [
    "## Algorithm Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "086032f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: \n",
      "def can_arrange(arr):\n",
      "    \"\"\"Create a function which returns the largest index of an element which\n",
      "    is not greater than or equal to the element immediately preceding it. If\n",
      "    no such element exists then return -1. The given array will not contain\n",
      "    duplicate values.\n",
      "\n",
      "    Examples:\n",
      "    can_arrange([1,2,4,3,5]) = 3\n",
      "    can_arrange([1,2,3]) = -1\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: \n",
      "def max_fill(grid, capacity):\n",
      "    import math\n",
      "    \"\"\"\n",
      "    You are given a rectangular grid of wells. Each row represents a single well,\n",
      "    and each 1 in a row represents a single unit of water.\n",
      "    Each well has a corresponding bucket that can be used to extract water from it, \n",
      "    and all buckets have the same capacity.\n",
      "    Your task is to use the buckets to empty the wells.\n",
      "    Output the number of times you need to lower the buckets.\n",
      "\n",
      "    Example 1:\n",
      "        Input: \n",
      "            grid : [[0,0,1,0], [0,1,0,0], [1,1,1,1]]\n",
      "            bucket_capacity : 1\n",
      "        Output: 6\n",
      "\n",
      "    Example 2:\n",
      "        Input: \n",
      "            grid : [[0,0,1,1], [0,0,0,0], [1,1,1,1], [0,1,1,1]]\n",
      "            bucket_capacity : 2\n",
      "        Output: 5\n",
      "    \n",
      "    Example 3:\n",
      "        Input: \n",
      "            grid : [[0,0,0], [0,0,0]]\n",
      "            bucket_capacity : 5\n",
      "        Output: 0\n",
      "\n",
      "    Constraints:\n",
      "        * all wells have the same length\n",
      "        * 1 <= grid.length <= 10^2\n",
      "        * 1 <= grid[:,1].length <= 10^2\n",
      "        * grid[i][j] -> 0 | 1\n",
      "        * 1 <= capacity <= 10\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: \n",
      "def digits(n):\n",
      "    \"\"\"Given a positive integer n, return the product of the odd digits.\n",
      "    Return 0 if all digits are even.\n",
      "    For example:\n",
      "    digits(1)  == 1\n",
      "    digits(4)  == 0\n",
      "    digits(235) == 15\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: \n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: \n",
      "def check_dict_case(dict):\n",
      "    \"\"\"\n",
      "    Given a dictionary, return True if all keys are strings in lower \n",
      "    case or all keys are strings in upper case, else return False.\n",
      "    The function should return False is the given dictionary is empty.\n",
      "    Examples:\n",
      "    check_dict_case({\"a\":\"apple\", \"b\":\"banana\"}) should return True.\n",
      "    check_dict_case({\"a\":\"apple\", \"A\":\"banana\", \"B\":\"banana\"}) should return False.\n",
      "    check_dict_case({\"a\":\"apple\", 8:\"banana\", \"a\":\"apple\"}) should return False.\n",
      "    check_dict_case({\"Name\":\"John\", \"Age\":\"36\", \"City\":\"Houston\"}) should return False.\n",
      "    check_dict_case({\"STATE\":\"NC\", \"ZIP\":\"12345\" }) should return True.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: from typing import List\n",
      "\n",
      "\n",
      "def filter_by_prefix(strings: List[str], prefix: str) -> List[str]:\n",
      "    \"\"\" Filter an input list of strings only for ones that start with a given prefix.\n",
      "    >>> filter_by_prefix([], 'a')\n",
      "    []\n",
      "    >>> filter_by_prefix(['abc', 'bcd', 'cde', 'array'], 'a')\n",
      "    ['abc', 'array']\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: \n",
      "def right_angle_triangle(a, b, c):\n",
      "    '''\n",
      "    Given the lengths of the three sides of a triangle. Return True if the three\n",
      "    sides form a right-angled triangle, False otherwise.\n",
      "    A right-angled triangle is a triangle in which one angle is right angle or \n",
      "    90 degree.\n",
      "    Example:\n",
      "    right_angle_triangle(3, 4, 5) == True\n",
      "    right_angle_triangle(1, 2, 3) == False\n",
      "    '''\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: \n",
      "\n",
      "def remove_vowels(text):\n",
      "    \"\"\"\n",
      "    remove_vowels is a function that takes string and returns string without vowels.\n",
      "    >>> remove_vowels('')\n",
      "    ''\n",
      "    >>> remove_vowels(\"abcdef\\nghijklm\")\n",
      "    'bcdf\\nghjklm'\n",
      "    >>> remove_vowels('abcdef')\n",
      "    'bcdf'\n",
      "    >>> remove_vowels('aaaaa')\n",
      "    ''\n",
      "    >>> remove_vowels('aaBAA')\n",
      "    'B'\n",
      "    >>> remove_vowels('zbcd')\n",
      "    'zbcd'\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: \n",
      "def words_string(s):\n",
      "    \"\"\"\n",
      "    You will be given a string of words separated by commas or spaces. Your task is\n",
      "    to split the string into words and return an array of the words.\n",
      "    \n",
      "    For example:\n",
      "    words_string(\"Hi, my name is John\") == [\"Hi\", \"my\", \"name\", \"is\", \"John\"]\n",
      "    words_string(\"One, two, three, four, five, six\") == [\"One\", \"two\", \"three\", \"four\", \"five\", \"six\"]\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "Error generating response for prompt: f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: \n",
      "def order_by_points(nums):\n",
      "    \"\"\"\n",
      "    Write a function which sorts the given list of integers\n",
      "    in ascending order according to the sum of their digits.\n",
      "    Note: if there are several items with similar sum of their digits,\n",
      "    order them based on their index in original list.\n",
      "\n",
      "    For example:\n",
      "    >>> order_by_points([1, 11, -1, -11, -12]) == [-1, -11, 1, -12, 11]\n",
      "    >>> order_by_points([]) == []\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:\n",
      "'T5ForConditionalGeneration' object has no attribute 'generate_content'\n",
      "[{'task_id': 'HumanEval/135', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}, {'task_id': 'HumanEval/115', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}, {'task_id': 'HumanEval/131', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}, {'task_id': 'HumanEval/55', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}, {'task_id': 'HumanEval/95', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}, {'task_id': 'HumanEval/29', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}, {'task_id': 'HumanEval/157', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}, {'task_id': 'HumanEval/51', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}, {'task_id': 'HumanEval/101', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}, {'task_id': 'HumanEval/145', 'prompt': \"f'Implement the function function_name that solves the following problem:\\n\\nProblem statement: {Problem description}\\n\\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\"}]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'HUMAN_EVAL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m optimal_prompt(Po)\n",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m, in \u001b[0;36moptimal_prompt\u001b[1;34m(Po)\u001b[0m\n\u001b[0;32m      4\u001b[0m prompt_candidates \u001b[38;5;241m=\u001b[39m C\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt_candidates)\n\u001b[1;32m----> 6\u001b[0m optimal_prompt \u001b[38;5;241m=\u001b[39m best_prompt(prompt_candidates) \u001b[38;5;66;03m# Find the best prompt from the final beam\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimal_prompt\n",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m, in \u001b[0;36mbest_prompt\u001b[1;34m(prompt_candidates, sample_file, problem_file)\u001b[0m\n\u001b[0;32m     16\u001b[0m write_jsonl(filename\u001b[38;5;241m=\u001b[39msample_file, data\u001b[38;5;241m=\u001b[39mprompt_candidates)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Evaluate functional correctness\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pass_at_k \u001b[38;5;241m=\u001b[39m evaluate_functional_correctness(sample_file, k\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m], n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.0\u001b[39m, problem_file\u001b[38;5;241m=\u001b[39mproblem_file)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load problems for error analysis (optional)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m problems \u001b[38;5;241m=\u001b[39m read_problems(problem_file)\n",
      "File \u001b[1;32m~\\Documents\\UNI\\7th semester\\Deep Learning and Neural Network\\Mini Project\\Main\\ProTeGi\\HumanEval\\evaluators\\evaluation.py:51\u001b[0m, in \u001b[0;36mevaluate_functional_correctness\u001b[1;34m(sample_file, k, n_workers, timeout, problem_file)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_functional_correctness\u001b[39m(\n\u001b[0;32m     40\u001b[0m     sample_file: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     41\u001b[0m     k: List[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     problem_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m HUMAN_EVAL,\n\u001b[0;32m     45\u001b[0m ):\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Evaluates the functional correctness of generated samples, and writes\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    results to f\"{sample_file}_results.jsonl.gz\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     problems \u001b[38;5;241m=\u001b[39m read_problems(problem_file)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Check the generated samples against test suites.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mn_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\human_eval\\data.py:12\u001b[0m, in \u001b[0;36mread_problems\u001b[1;34m(evalset_file)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_problems\u001b[39m(evalset_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m HUMAN_EVAL) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Dict]:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]: task \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m stream_jsonl(evalset_file)}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\human_eval\\data.py:26\u001b[0m, in \u001b[0;36mstream_jsonl\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     24\u001b[0m                     \u001b[38;5;28;01myield\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fp:\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39misspace() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m line):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'HUMAN_EVAL'"
     ]
    }
   ],
   "source": [
    "prompt = optimal_prompt(Po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85ae23fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f'Implement the function function_name that solves the following problem:\n",
      "\n",
      "Problem statement: {Problem description}\n",
      "\n",
      "Function signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb4004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
