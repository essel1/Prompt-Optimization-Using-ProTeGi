{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16edffc0-c5f5-44c8-9cc7-f5e31efc3a16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports and accessing LLM, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "189b7c4f-3487-49df-b9d1-ccd2ec62c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "# Add the 'evaluation' folder to the Python path\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(current_dir, 'evaluation'))\n",
    "sys.path.append(os.path.join(current_dir, 'data'))\n",
    "\n",
    "from generate_gradient import generate_textual_gradient\n",
    "from evaluators.evaluation import evaluate_functional_correctness\n",
    "from evaluators.evaluation import read_problems,stream_jsonl, write_jsonl\n",
    "\n",
    "import google.generativeai as genai\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfaafb37-1e8b-455a-949b-898ccd04f614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'GenerativeModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m gemini_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIzaSyC8TjivmylsAQPaCyOhdaEWS1Ktt-8QqSQ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39mgemini_key)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerativeModel\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf://datasets/openai/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'google.generativeai' has no attribute 'GenerativeModel'"
     ]
    }
   ],
   "source": [
    "gemini_key = \"AIzaSyC8TjivmylsAQPaCyOhdaEWS1Ktt-8QqSQ\"\n",
    "genai.configure(api_key=gemini_key)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/openai/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2e6f9-2123-45bd-89d3-109c23e37e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdd24b-d9dd-4c3e-b976-867b9e73f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(dataframe) -> List[dict]:\n",
    "    responses = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        task_id = row.get('task_id')\n",
    "        prompt = row.get('prompt')\n",
    "        try:\n",
    "            generated_soln = model.generate_content(prompt)\n",
    "            if generated_soln.candidates[0].finish_reason == 1:\n",
    "                print(f\"{task_id} done\")\n",
    "                responses.append(\n",
    "                {\n",
    "                    \"task_id\": task_id,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"solution\": solution,\n",
    "                    \"generated_output\": generated_soln.text,\n",
    "                }\n",
    "            )\n",
    "            elif generated_soln.candidates[0].finish_reason == 4:\n",
    "                print(f\"{task_id} generating copyrighted material\")\n",
    "                failed_responses.append(\n",
    "                {\n",
    "                    \"task_id\": task_id,\n",
    "                    \"prompt\" : prompt,\n",
    "                    \"solution\" : solution,\n",
    "                    \"failed_reason\": \"Reciting Copyrighted material\"\n",
    "                }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response for prompt: {prompt}\\n{e}\")\n",
    "            responses.append({\"task_id\":row.task_id, \"prompt\": prompt, \"generated_output\": \"\"})\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca5c72-955d-42ad-91ff-7721fe120ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['prompt'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eff1f1-41fa-4ac9-900a-ed22c2da8366",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Defining Po, Dtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb216774-0ac5-4721-9270-79c5d8cafcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Po = \"\"\"f'Implement the function function_name that solves the following problem:\n",
    "\n",
    "Problem statement: {Problem description}\n",
    "\n",
    "Function signature: def function_name(*args) -> return_type:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563dc55-1653-4cad-87fe-ae92e96045df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtr = df[[\"task_id\",\"prompt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917492c1-db0f-4072-abb5-6fd0ca1f168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ecd6d-209c-4b77-8eb1-5d0068f59cab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_to_dict(df) -> List[dict]:\n",
    "    return df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287b6bc-efe1-4f44-bda8-36f0ad555abe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Main Function (ProTeGi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a5801-bfa5-497a-a5d4-d11fb7d39564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_prompt(Po):\n",
    "    C = []\n",
    "    C.extend(expand(Po))\n",
    "    prompt_candidates = set(C)\n",
    "    \n",
    "    optimal_prompt = best_prompt(prompt_candidates) # Find the best prompt from the final beam\n",
    "    return optimal_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac81559-32a5-4a8d-8e02-d2c2fe060160",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Expansion Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e158d",
   "metadata": {},
   "source": [
    "### Loading the transformer model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c68fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Files in model folder:\", os.listdir(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db297612",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../CodeT5/fine_tuned_codet5\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ac8ba",
   "metadata": {},
   "source": [
    "### Expansion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde00f50-27c0-48f6-87ef-b2ef6e17f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_minibatch(df: pd.DataFrame, batch_size: int) -> pd.DataFrame:\n",
    "    return df.sample(n=batch_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f286cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_error(input_text, model, tokenizer, device):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate output using the model\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=256,  # Adjust this as needed\n",
    "        temperature=0.7,  # Sampling temperature for diversity\n",
    "        num_return_sequences=1,  # Number of responses to generate\n",
    "        top_k=50,  # Use top-k sampling\n",
    "        top_p=0.95  # Use nucleus sampling\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(Po,Dtr):\n",
    "    batch_size = 10\n",
    "    D_mini_batch = sample_minibatch(Dtr,batch_size)\n",
    "    \n",
    "    def fill_prompt(row):\n",
    "        problem_description = row.get(\"prompt\", \"\")\n",
    "        # Insert them into Po\n",
    "        merged_prompt = (\n",
    "            Po.replace(\"{Problem description}\", problem_description)\n",
    "        )\n",
    "        return merged_prompt\n",
    "    \n",
    "    D_mini_batch[\"prompt\"] = D_mini_batch_df.apply(fill_prompt, axis=1)\n",
    "    \n",
    "    responses = generate_responses(D_mini_batch_df)\n",
    "    \n",
    "    prompt_candidates = []\n",
    "    \n",
    "    for response in responses:\n",
    "        problem_id = response.get(\"task_id\")\n",
    "        generated_output = response.get(\"generated_output\")\n",
    "        expected_output = problems[problem_id][\"canonical_solution\"]\n",
    "        input_text = (\n",
    "            f\"\"\"Generated_output: {generated_output}, expected_output: {expected_output} \"\"\"\n",
    "            f\"\"\"What is the error or difference?\"\"\"\n",
    "        )\n",
    "        error = generate_error(input_text, model, tokenizer, device) \n",
    "        gradient = generate_textual_gradient(error, use_similarity=True)\n",
    "        edited_prompt = (Po + \"/t\" + gradient)\n",
    "        prompt_candidates.append({\n",
    "        \"task_id\":problem_id,\n",
    "        \"prompt\": edited_prompt\n",
    "        }\n",
    "        )\n",
    "        \n",
    "    return prompt_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d2154-f46d-4e43-b48b-1b4ebb621806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_prompt(prompt_candidates, sample_file=\"sample_file.jsonl\", problem_file=\"HUMAN_EVAL\"):\n",
    "    \"\"\"\n",
    "    Evaluates the pass@k for each prompt candidate and returns the prompt with the highest pass@k value.\n",
    "    \n",
    "    Args:\n",
    "        prompt_candidates (list): List of dictionaries with the format {\"task_id\": problem_id, \"prompt\": edited_prompt}.\n",
    "        sample_file (str): Path to save generated responses for evaluation.\n",
    "        problem_file (str): Path to the problem definitions.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The prompt candidate with the highest pass@k value.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Write prompt candidates to the sample file\n",
    "    write_jsonl(filename=sample_file, data=prompt_candidates)\n",
    "\n",
    "    # Evaluate functional correctness\n",
    "    pass_at_k = evaluate_functional_correctness(sample_file, k=[1, 10, 100], n_workers=4, timeout=3.0, problem_file=problem_file)\n",
    "\n",
    "    # Load problems for error analysis (optional)\n",
    "    problems = read_problems(problem_file)\n",
    "\n",
    "    # Analyze errors and collect pass@k values\n",
    "    best_prompt = None\n",
    "    best_pass_at_k = -1  # Initialize with the lowest possible value\n",
    "\n",
    "    for candidate in prompt_candidates:\n",
    "        task_id = candidate.get(\"task_id\")\n",
    "        prompt = candidate.get(\"prompt\")\n",
    "        \n",
    "        # Filter pass@k for this specific task\n",
    "        task_pass_at_k = {key: pass_at_k[key] for key in pass_at_k if str(task_id) in key}\n",
    "\n",
    "        # Compute average pass@k (or select the metric you want to optimize)\n",
    "        avg_pass_at_k = sum(task_pass_at_k.values()) / len(task_pass_at_k) if task_pass_at_k else 0\n",
    "\n",
    "        # Update the best prompt based on pass@k\n",
    "        if avg_pass_at_k > best_pass_at_k:\n",
    "            best_pass_at_k = avg_pass_at_k\n",
    "            best_prompt = {\n",
    "                \"task_id\": task_id,\n",
    "                \"prompt\": prompt,\n",
    "                \"pass@k\": task_pass_at_k,\n",
    "                \"average_pass@k\": avg_pass_at_k\n",
    "            }\n",
    "\n",
    "    return best_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79d1b8",
   "metadata": {},
   "source": [
    "## Algorithm Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086032f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = optimal_prompt(Po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb4004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
