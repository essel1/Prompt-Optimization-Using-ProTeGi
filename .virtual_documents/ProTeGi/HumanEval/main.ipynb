


import pandas as pd
import google.generativeai as genai
import random
from typing import List

import sys
import os
# from dotenv import load_dotenv
# Add the 'evaluation' folder to the Python path
current_dir = os.getcwd()
sys.path.append(os.path.join(current_dir, 'evaluation'))
sys.path.append(os.path.join(current_dir, 'data'))

from generate_gradient import generate_textual_gradient
from evaluators.evaluation import evaluate_functional_correctness
from evaluators.evaluation import read_problems,stream_jsonl, write_jsonl

import google.generativeai as genai
from transformers import RobertaTokenizer, T5ForConditionalGeneration
import torch


gemini_key = "AIzaSyC8TjivmylsAQPaCyOhdaEWS1Ktt-8QqSQ"
genai.configure(api_key=gemini_key)
model = genai.GenerativeModel('gemini-1.5-flash')


df = pd.read_parquet("hf://datasets/openai/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet")


df


def generate_responses(dataframe) -> List[dict]:
    responses = []
    for _, row in dataframe.iterrows():
        task_id = row.get('task_id')
        prompt = row.get('prompt')
        try:
            generated_soln = model.generate_content(prompt)
            if generated_soln.candidates[0].finish_reason == 1:
                print(f"{task_id} done")
                responses.append(
                {
                    "task_id": task_id,
                    "prompt": prompt,
                    "solution": solution,
                    "generated_output": generated_soln.text,
                }
            )
            elif generated_soln.candidates[0].finish_reason == 4:
                print(f"{task_id} generating copyrighted material")
                failed_responses.append(
                {
                    "task_id": task_id,
                    "prompt" : prompt,
                    "solution" : solution,
                    "failed_reason": "Reciting Copyrighted material"
                }
                )
        except Exception as e:
            print(f"Error generating response for prompt: {prompt}\n{e}")
            responses.append({"task_id":row.task_id, "prompt": prompt, "generated_output": ""})
    return responses


prompt = "f'Implement the function function_name that solves the following problem:\n\nProblem statement: {Problem description}\n\nFunction signature: def function_name(*args) -> return_type:/tFocus on syntax correctness. Fix compilation issues first. Align the solution more closely with the reference's approach or logic. The error is moderately distant from the ideal solution. Further improvements needed."


print(df['prompt'].iloc[0])





Po = """f'Implement the function function_name that solves the following problem:

Problem statement: {Problem description}

Function signature: def function_name(*args) -> return_type:"""


Dtr = df[["task_id","prompt"]]


Dtr


def df_to_dict(df) -> List[dict]:
    return df.to_dict(orient="records")





def optimal_prompt(Po):
    C = []
    C.extend(expand(Po,Dtr))
    prompt_candidates = C
    print(prompt_candidates)
    optimal_prompt = best_prompt(prompt_candidates) # Find the best prompt from the final beam
    return optimal_prompt








model_path = "../../CodeT5/fine_tuned_codet5"
tokenizer = RobertaTokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)





def sample_minibatch(df: pd.DataFrame, batch_size: int) -> pd.DataFrame:
    return df.sample(n=batch_size, random_state=42)


def generate_error(input_text, model, tokenizer, device):
    # Tokenize the input text
    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True).to(device)

    # Generate output using the model
    outputs = model.generate(
        inputs.input_ids,
        max_length=256,  # Adjust this as needed
        temperature=0.7,  # Sampling temperature for diversity
        num_return_sequences=1,  # Number of responses to generate
        top_k=50,  # Use top-k sampling
        top_p=0.95  # Use nucleus sampling
    )

    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text


def expand(Po,Dtr):
    batch_size = 10
    D_mini_batch = sample_minibatch(Dtr,batch_size)
    
    def fill_prompt(row):
        problem_description = row.get("prompt", "")
        # Insert them into Po
        merged_prompt = (
            Po.replace("{Problem description}", problem_description)
        )
        return merged_prompt
    problems = read_problems()
    D_mini_batch["prompt"] = D_mini_batch.apply(fill_prompt, axis=1)
    
    responses = generate_responses(D_mini_batch)
    
    prompt_candidates = []
    
    for response in responses:
        problem_id = response.get("task_id")
        generated_output = response.get("generated_output")
        expected_output = problems[problem_id]["canonical_solution"]
        input_text = (
            f"""Generated_output: {generated_output}, expected_output: {expected_output} """
            f"""What is the error or difference?"""
        )
        error = generate_error(input_text, model, tokenizer, device) 
        gradient = generate_textual_gradient(error, use_similarity=True)
        edited_prompt = (Po + "/t" + gradient)
        prompt_candidates.append({
        "task_id":problem_id,
        "prompt": edited_prompt
        }
        )
        
    return prompt_candidates


def best_prompt(prompt_candidates, sample_file="sample_file.jsonl", problem_file="HUMAN_EVAL"):
    """
    Evaluates the pass@k for each prompt candidate and returns the prompt with the highest pass@k value.
    
    Args:
        prompt_candidates (list): List of dictionaries with the format {"task_id": problem_id, "prompt": edited_prompt}.
        sample_file (str): Path to save generated responses for evaluation.
        problem_file (str): Path to the problem definitions.
    
    Returns:
        dict: The prompt candidate with the highest pass@k value.
    """
    from collections import defaultdict

    # Write prompt candidates to the sample file
    write_jsonl(filename=sample_file, data=prompt_candidates)

    # Evaluate functional correctness
    pass_at_k = evaluate_functional_correctness(sample_file, k=[1, 10, 100], n_workers=4, timeout=3.0, problem_file=problem_file)

    # Load problems for error analysis (optional)
    problems = read_problems(problem_file)

    # Analyze errors and collect pass@k values
    best_prompt = None
    best_pass_at_k = -1  # Initialize with the lowest possible value

    for candidate in prompt_candidates:
        task_id = candidate.get("task_id")
        prompt = candidate.get("prompt")
        
        # Filter pass@k for this specific task
        task_pass_at_k = {key: pass_at_k[key] for key in pass_at_k if str(task_id) in key}

        # Compute average pass@k (or select the metric you want to optimize)
        avg_pass_at_k = sum(task_pass_at_k.values()) / len(task_pass_at_k) if task_pass_at_k else 0

        # Update the best prompt based on pass@k
        if avg_pass_at_k > best_pass_at_k:
            best_pass_at_k = avg_pass_at_k
            best_prompt = {
                "task_id": task_id,
                "prompt": prompt,
                "pass@k": task_pass_at_k,
                "average_pass@k": avg_pass_at_k
            }

    return best_prompt





prompt = optimal_prompt(Po)


print(prompt)



