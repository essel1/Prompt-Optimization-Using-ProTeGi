import json
import pandas as pd
import numpy as np
from datasets import load_dataset


# Load the two JSON files
with open('./results/0-3027.json', 'r') as f1, open('./results/3028-5000.json', 'r') as f2:
    data1 = json.load(f1)
    data2 = json.load(f2)

# Merge the dictionaries
merged_data = {**data1, **data2}

# Save the merged data into a new JSON file
with open('all_results.json', 'w') as f_out:
    json.dump(merged_data, f_out, indent=4)



len(merged_data)





def generate_error_dict(bleu_json_path, accuracy_json_path, output_json_path=None):
    """
    Reads BLEU and accuracy data, then generates a dictionary of dictionaries
    for each problem_id (instance_id). This structure can be used in subsequent
    scripts to create textual summaries.

    :param bleu_json_path: Path to a JSON file with BLEU scores.
        Example format:
            {
                "0": [64.1, 22.4],
                "1": [61.5, 20.3],
                "2": [75.2, 30.0],
                ...
            }
        Where each key is a stringified instance ID, and each value is
        a list of BLEU scores, e.g. [bleu_for_n_grams, bleu_for_instance].
    
    :param accuracy_json_path: Path to a JSON file with accuracy/test outcomes.
        Example format:
            {
                "0": [[True, False, -2, False]],
                "1": [[False, False, True]],
                "2": [[True, True, True]],
                ...
            }
        Where each key is a stringified instance ID, and each value is a
        list of lists. Each inner list represents outcomes for each test case
        in a single generated solution:
            - -2   -> Compile error
            - -1   -> Runtime error
            - True  -> Passed test case
            - False -> Failed test case

    :param output_json_path: Optional path to write the dictionary of dictionaries as JSON.
    :return: A dict where each key is a string (instance_id) and value is a dict with:
                {
                    "problem_id": <instance_id>,
                    "bleu_n_gram": float or None,
                    "bleu_instance": float or None,
                    "compile_error_count": int,
                    "runtime_error_count": int,
                    "passed_count": int,
                    "failed_count": int
                }
    """

    # 1. Load BLEU scores
    with open(bleu_json_path, 'r') as f_bleu:
        bleu_data = json.load(f_bleu)
    
    # 2. Load accuracy data
    with open(accuracy_json_path, 'r') as f_acc:
        accuracy_data = json.load(f_acc)
    
    # 3. Construct the dictionary of dictionaries
    error_dict = {}

    # Use the union of keys from both datasets in case there's a mismatch
    all_instance_ids = set(bleu_data.keys()).union(set(accuracy_data.keys()))

    for instance_id in all_instance_ids:
        # Retrieve BLEU scores (with fallback if missing)
        bleu_scores = bleu_data.get(instance_id, [None, None])
        bleu_n_gram = bleu_scores[0] if len(bleu_scores) > 0 else None
        bleu_instance = bleu_scores[1] if len(bleu_scores) > 1 else None
        
        # Retrieve accuracy data (take the first solution's results if multiple)
        accuracy_list_of_lists = accuracy_data.get(instance_id, [])
        test_outcomes = accuracy_list_of_lists[0] if accuracy_list_of_lists else []
        
        # Count each outcome
        compile_error_count = 0
        runtime_error_count = 0
        passed_count = 0
        failed_count = 0
        
        for outcome in test_outcomes:
            if outcome == True:
                passed_count += 1
            elif outcome == False:
                failed_count += 1
            elif outcome == -2:
                compile_error_count += 1
            elif outcome == -1:
                runtime_error_count += 1
        
        # Prepare the dictionary entry
        error_dict[instance_id] = {
            "problem_id": instance_id,
            "bleu_n_gram": bleu_n_gram,
            "bleu_instance": bleu_instance,
            "compile_error_count": compile_error_count,
            "runtime_error_count": runtime_error_count,
            "passed_count": passed_count,
            "failed_count": failed_count
        }
    
    # 4. Optionally write to JSON
    if output_json_path:
        with open(output_json_path, 'w') as f_out:
            json.dump(error_dict, f_out, indent=2)
    
    return error_dict





if __name__ == "__main__":
    # Example usage:
    bleu_json_path = "./results/all_bleu_results.json"       # Path to your BLEU data
    accuracy_json_path = "./results/all_results.json" # Path to your accuracy data
    output_json_path = "./results/error_dict.json"      # Where to store the dictionary of dictionaries

    error_dict = generate_error_dict(
        bleu_json_path,
        accuracy_json_path,
        output_json_path=output_json_path
    )
    
    # Print a sample of the data structure
    for key in list(error_dict.keys())[:5]:
        print(key, ":", error_dict[key])


import json

def generate_prioritized_error_info(bleu_json_path, accuracy_json_path, output_json_path=None):
    """
    Reads BLEU and accuracy data, then generates a dictionary of dictionaries
    reflecting a prioritized approach to code errors:

    Priority:
      1) Compile errors
      2) Runtime errors
      3) Test pass/fail
      4) BLEU similarity

    The final text summary avoids specific numeric references (like '1 compile error')
    and instead provides a coarse description.

    :param bleu_json_path: Path to a JSON file with BLEU scores.
        Example format:
            {
                "0": [64.1, 22.4],
                "1": [61.5, 20.3],
                ...
            }
        Each key is a stringified problem ID. Value is [bleu_for_n_gram, bleu_for_instance].

    :param accuracy_json_path: Path to a JSON with accuracy/test outcomes.
        Example format:
            {
                "0": [[True, False, -2, False]],
                "1": [[False, False, True]],
                ...
            }
        Each key is a stringified problem ID. Each value is a list of lists; each inner list
        represents results for a single solution:
          - -2 -> Compile error
          - -1 -> Runtime error
          - True -> Passed test case
          - False -> Failed test case

    :param output_json_path: Optional path to store the dictionary of dictionaries as JSON.

    :return: Dict of the form:
        {
          "0": {
              "problem_id": "0",
              "internal_counts": {
                  "compile_errors": int,
                  "runtime_errors": int,
                  "passed_tests": int,
                  "failed_tests": int,
                  "total_tests": int
              },
              "error_category": <one of: "compile_error", "runtime_error", "no_pass",
                                 "partial_pass", "all_pass", "no_tests">,
              "bleu_category": <"very_close", "close", "far", "unavailable">,
              "summary": <short textual statement with no numeric detail>
          },
          "1": { ... },
          ...
        }
    """

    # Load BLEU data
    with open(bleu_json_path, 'r') as f_bleu:
        bleu_data = json.load(f_bleu)

    # Load Accuracy data
    with open(accuracy_json_path, 'r') as f_acc:
        accuracy_data = json.load(f_acc)

    # The final container
    prioritized_data = {}

    # Collect all problem IDs from both data sources
    all_ids = set(bleu_data.keys()).union(accuracy_data.keys())

    for problem_id in all_ids:
        # ---- 1) Gather raw data ----
        # BLEU
        bleu_scores = bleu_data.get(problem_id, [None, None])
        bleu_n_gram = bleu_scores[0]
        bleu_instance = bleu_scores[1]

        # Accuracy
        # By default, consider only the FIRST solution in accuracy_data for summary
        # (adjust if your workflow is different)
        solution_outcomes = []
        if problem_id in accuracy_data and len(accuracy_data[problem_id]) > 0:
            solution_outcomes = accuracy_data[problem_id][0]

        # Count compile errors, runtime errors, pass/fails
        compile_errors = 0
        runtime_errors = 0
        passed_tests = 0
        failed_tests = 0

        for outcome in solution_outcomes:
            if outcome == -2:
                compile_errors += 1
            elif outcome == -1:
                runtime_errors += 1
            elif outcome is True:
                passed_tests += 1
            elif outcome is False:
                failed_tests += 1

        total_tests = len(solution_outcomes)

        # ---- 2) Determine coarse categories based on priority logic ----
        # Highest priority: compile error
        if compile_errors > 0:
            error_category = "compile_error"
        # Next priority: runtime error
        elif runtime_errors > 0:
            error_category = "runtime_error"
        else:
            # If code compiles and runs, we check test results
            if total_tests == 0:
                # No tests available => cannot confirm pass/fail
                error_category = "no_tests"
            else:
                # Some tests exist
                if passed_tests == total_tests:
                    error_category = "all_pass"
                elif passed_tests == 0:
                    error_category = "no_pass"
                else:
                    error_category = "partial_pass"

        # BLEU categorization (only if we have numeric data)
        if bleu_n_gram is None or bleu_instance is None:
            bleu_category = "unavailable"
        else:
            avg_bleu = (bleu_n_gram + bleu_instance) / 2.0
            if avg_bleu >= 60:
                bleu_category = "very_close"
            elif avg_bleu >= 30:
                bleu_category = "close"
            else:
                bleu_category = "far"

        # ---- 3) Build a text summary that avoids numeric references ----
        # Summaries reflect priority:
        #  - If compile_error => highlight "fails to compile"
        #  - If runtime_error => highlight "crashes at runtime"
        #  - Otherwise describe pass/fail
        #  - Then mention BLEU in a coarse sense

        summary_parts = []

        # Error portion
        if error_category == "compile_error":
            summary_parts.append("The code fails to compile.")
            summary_parts.append("Fix compilation issues first.")
        elif error_category == "runtime_error":
            summary_parts.append("The code compiles but crashes at runtime.")
            summary_parts.append("Fix runtime errors next.")
        elif error_category == "no_tests":
            summary_parts.append("The code runs, but there are no test results available.")
        elif error_category == "all_pass":
            summary_parts.append("The code compiles and passes all tests.")
        elif error_category == "no_pass":
            summary_parts.append("The code compiles but does not pass any tests.")
        elif error_category == "partial_pass":
            summary_parts.append("The code compiles and passes some tests but not all.")

        # BLEU portion (coarse mention)
        if bleu_category == "very_close":
            summary_parts.append("It appears very similar to the reference solution.")
        elif bleu_category == "close":
            summary_parts.append("It has moderate similarity to the reference.")
        elif bleu_category == "far":
            summary_parts.append("It is significantly different from the reference.")
        # if "unavailable", we just omit mention of BLEU

        final_summary = " ".join(summary_parts).strip()

        # ---- 4) Store everything in final structure ----
        prioritized_data[problem_id] = {
            "problem_id": problem_id,
            "internal_counts": {
                "compile_errors": compile_errors,
                "runtime_errors": runtime_errors,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "total_tests": total_tests
            },
            "error_category": error_category,
            "bleu_category": bleu_category,
            "summary": final_summary
        }

    # ---- 5) Optionally save to a JSON file ----
    if output_json_path:
        with open(output_json_path, 'w') as f_out:
            json.dump(prioritized_data, f_out, indent=2)

    return prioritized_data


if __name__ == "__main__":
    """
    Example usage. Adjust file paths as needed, and run:
        python script_name.py
    """
    # Paths to your data
    bleu_json_path = "./results/all_bleu_results.json"
    accuracy_json_path = "./results/all_results.json"
    output_json_path = "./results/textual_errors.json"

    data = generate_prioritized_error_info(
        bleu_json_path,
        accuracy_json_path,
        output_json_path=output_json_path
    )

    # Print a few samples to see the final structure + summary
    for k in list(data.keys())[:5]:
        print(k, "=>", data[k])









with open("./results/textual_errors.json") as file:
    textual_errors = json.load(file)

len(textual_errors)


textual_errors


df_data = []
for key, value in textual_errors.items():
    df_data.append({'problem_id': value['problem_id'], 'summary': value['summary']})

error_df = pd.DataFrame(df_data)
print(error_df.head(5))



apps = load_dataset("codeparrot/apps",split="train")


apps_df = apps.to_pandas()


apps_df


apps_df = apps_df[["problem_id","question","solutions"]]


apps_df


with open("../Responses/response.json") as file:
    generated_response = json.load(file)


generated_response[1]


error_df_1 = pd.DataFrame(generated_response)
error_df_1.head(5)


error_df_1 = error_df_1.rename(columns={"task_id":"problem_id"})
error_df_1.head(5)


error_df.head(5)


type(error_df_1["problem_id"][1])


type(error_df["problem_id"][1])


error_df["problem_id"] = error_df['problem_id'].astype(np.int64)


type(error_df["problem_id"][1])


error_summary_df = pd.merge(error_df, error_df_1, on='problem_id', how='inner')
error_summary_df.head(5)


len(error_summary_df)


error_summary_df.tail(5)


error_summary_df.to_csv("./results/error_summary.csv",index=False)
